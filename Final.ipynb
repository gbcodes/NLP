{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>NLP</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на данные и сделаем импорты необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Gaelic translation \\n\\nHi, don't suppose you c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Hey dick \\nYou don't know what is copyright so...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\"\\n\\nAm I correct in thinking that you are ref...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>== Notable Alumni == \\n\\n Why was this section...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I have already been sent this message about va...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                       comment_text  toxic  severe_toxic  \\\n",
       "0   0  Gaelic translation \\n\\nHi, don't suppose you c...      0             0   \n",
       "1   1  Hey dick \\nYou don't know what is copyright so...      1             0   \n",
       "2   2  \"\\n\\nAm I correct in thinking that you are ref...      0             0   \n",
       "3   3  == Notable Alumni == \\n\\n Why was this section...      0             0   \n",
       "4   4  I have already been sent this message about va...      1             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \n",
       "0        0       0       0              0  \n",
       "1        1       0       1              0  \n",
       "2        0       0       0              0  \n",
       "3        0       0       0              0  \n",
       "4        1       0       1              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey dick \\nYou don't know what is copyright so shutup and get out of here, don't write stupid bullshit on my talk page. Dhivehi language is Dhivehi language not your bullshit nonsese.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.comment_text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Избавимся от всякого хлама)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.comment_text = train.comment_text.apply(lambda x: re.sub('[!@#$;:%-&?<>*)(=_*/+|}{.,]','',x))\n",
    "train.comment_text = train.comment_text.apply(lambda x: re.sub('\\d+','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Notable Alumni  \\n\\n Why was this section all-but-deleted US has many notable allumni but currently the only one listed is a teacher who just happened to pitch for the Detroit Tigers a long long time ago in a galaxy far far away For one his son Michael Seelbach currently has a lead role in the Chicago production of Wicked I understand that it was often vandalized by students of the school but does that merit the almost complete deletion of it ps it looks like it was deleted a l-o-o-o-o-ng time ago'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.comment_text[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее сделаем токенизацию и лемматизацию вручную, с помощью `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ilya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "train['new_comment'] = train.comment_text.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое для тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test.comment_text = test.comment_text.apply(lambda x: re.sub('[!@#$;:%-&?<>*)(=_*/+}{|.,]','',x))\n",
    "test.comment_text = test.comment_text.apply(lambda x: re.sub('\\d+','',x))\n",
    "# test['new_comment'] = test.comment_text.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Векторизуем тексты, с помощью `tf-ifd`. Также здесь избавимся от стопслов и сделаем токенизацию дополнительно.\n",
    "Вообще токенизацию с помощью nltk можно было не делать, так как она производится тут, но я попробовал несколько вариантов. И делать токенизацию с лемматизацией с помощью nltk оказалось эффективнее, поэтому тут закомментируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True,\n",
    "                                   strip_accents='unicode', \n",
    "                                   stop_words='english', \n",
    "                                  analyzer='word',\n",
    "#                                   token_pattern=r'\\w{1,}',\n",
    "                                  ngram_range=(1,1),\n",
    "                                   max_features=10000,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для векторизации требуется, чтобы на входе был лист из строк, а у нас лист из листов, поэтому..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['new_comment'] = [str(x) for x in train.comment_text]\n",
    "test['new_comment'] = [str(x) for x in test.comment_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.new_comment = train.new_comment.apply(lambda x: x.replace('\\n', ''))\n",
    "test.new_comment = test.new_comment.apply(lambda x: x.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = pd.concat([train.new_comment, test.new_comment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alls = tfidf_vectorizer.fit(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents='unicode', sublinear_tf=True,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = tfidf_vectorizer.transform(train.new_comment)\n",
    "test_features = tfidf_vectorizer.transform(test.new_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделали веторизацию текста, теперь сделаем векторизацию символов в тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(2, 6),\n",
    "    max_features=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "char векторизацию здесь запускать не стал, тк слишком ресурсозатратная, а времени уже в обрез)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "char_vectorizer.fit(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char_features = char_vectorizer.transform(train.new_comment)\n",
    "test_char_features = char_vectorizer.transform(test.new_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "train_fin_features = hstack([train_char_features, train_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fin_features = hstack([test_char_features, test_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим лейблы для обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим отдельный дф для сабмита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = test.id\n",
    "for i in categories:\n",
    "    sub[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь самое интересное) Наверное, самое разумное сразу подумать об обучении с учителем. Пройдемся по тем методам, которые очень хорошо работают с вероятностью. В первую очередь: `Logistig Regression` и `Random Forest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "... Processing severe_toxic\n",
      "... Processing obscene\n",
      "... Processing threat\n",
      "... Processing insult\n",
      "... Processing identity_hate\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C=0.1, solver='sag')\n",
    "for category in categories:\n",
    "    print('... Processing {}'.format(category))\n",
    "    model.fit(train_features, train[category])\n",
    "    prediction = model.predict_proba(test_features)[:,1]\n",
    "    sub[category] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<150000x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3055756 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toxic'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9688289687112009"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = []\n",
    "for train_target in categories:\n",
    "    cv_score = np.mean(cross_val_score(model, train_features, train[train_target], cv=3, scoring='roc_auc')) \n",
    "    scores.append(cv_score)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.278040</td>\n",
       "      <td>0.006556</td>\n",
       "      <td>0.051493</td>\n",
       "      <td>0.003417</td>\n",
       "      <td>0.098566</td>\n",
       "      <td>0.007584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150001</td>\n",
       "      <td>0.034944</td>\n",
       "      <td>0.005216</td>\n",
       "      <td>0.023202</td>\n",
       "      <td>0.002938</td>\n",
       "      <td>0.018518</td>\n",
       "      <td>0.006263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150002</td>\n",
       "      <td>0.038319</td>\n",
       "      <td>0.004483</td>\n",
       "      <td>0.021228</td>\n",
       "      <td>0.002466</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.004869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150003</td>\n",
       "      <td>0.008351</td>\n",
       "      <td>0.003556</td>\n",
       "      <td>0.008114</td>\n",
       "      <td>0.002175</td>\n",
       "      <td>0.007408</td>\n",
       "      <td>0.003989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150004</td>\n",
       "      <td>0.059174</td>\n",
       "      <td>0.004122</td>\n",
       "      <td>0.027986</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.027352</td>\n",
       "      <td>0.006226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id     toxic  severe_toxic   obscene    threat    insult  identity_hate\n",
       "0  150000  0.278040      0.006556  0.051493  0.003417  0.098566       0.007584\n",
       "1  150001  0.034944      0.005216  0.023202  0.002938  0.018518       0.006263\n",
       "2  150002  0.038319      0.004483  0.021228  0.002466  0.020202       0.004869\n",
       "3  150003  0.008351      0.003556  0.008114  0.002175  0.007408       0.003989\n",
       "4  150004  0.059174      0.004122  0.027986  0.002476  0.027352       0.006226"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры для модели подобрал вручную. Получается очень неплохая точность, однако этого не достаточно для попадания в топ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "... Processing severe_toxic\n",
      "... Processing obscene\n",
      "... Processing threat\n",
      "... Processing insult\n",
      "... Processing identity_hate\n"
     ]
    }
   ],
   "source": [
    "rb = RandomForestClassifier()\n",
    "for category in categories:\n",
    "    print('... Processing {}'.format(category))\n",
    "    rb.fit(train_features, train[category])\n",
    "    prediction = rb.predict_proba(test_features)[:,1]\n",
    "    sub[category] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8668138616107502"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "for train_target in categories:\n",
    "    cv_score = np.mean(cross_val_score(rb, train_features, train[train_target], cv=3, scoring='roc_auc')) \n",
    "    scores.append(cv_score)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность получилась совсем не очень, подберем гиперпараметры на `GridCearchCV`, а также применим `OneVsRestClassifier`, так как имеем дело с miltilabel классификацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "  \"estimator__n_estimators\": [5, 50, 200],\n",
    "  \"estimator__max_depth\" : [None, 10,20],\n",
    "  \"estimator__min_samples_split\" : [2, 5, 10],\n",
    "}\n",
    "model_to_tune = OneVsRestClassifier(RandomForestClassifier(random_state=0,class_weight='auto'))\n",
    "model_tuned = GridSearchCV(model_to_tune, param_grid=params, scoring='f1',n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_ in categories:\n",
    "    y_this_class = (y_all_class==a_class)\n",
    "    model_to_tune = RandomForestClassifier(random_state=0,class_weight='auto')\n",
    "    model_tuned = GridSearchCV(model_to_tune, param_grid=params, scoring='f1',n_jobs=2)\n",
    "    model_tuned.fit(train_features, train[y_this_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также не стал запускать из-за плотной нехватки времени, но там все работает(можете проверить)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb = RandomForestClassifier(n_estimators=50, max_depth=20, min_samples_split=2)\n",
    "for category in categories:\n",
    "    print('... Processing {}'.format(category))\n",
    "    rb.fit(train_features, train[category])\n",
    "    prediction = rb.predict_proba(test_features)[:,1]\n",
    "    sub[category] = prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Random Forest` показал себя не с лучшей стороны,тк долго работает и точность выдает не самую наилучшую, даже после подбора параметров. Применим метод опорных векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC, SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "... Processing severe_toxic\n",
      "... Processing obscene\n",
      "... Processing threat\n",
      "... Processing insult\n",
      "... Processing identity_hate\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC(C=0.7, max_iter=10000)\n",
    "for category in categories:\n",
    "    print('... Processing {}'.format(category))\n",
    "    clf.fit(train_features, train[category])\n",
    "    prediction = clf.predict(test_features)\n",
    "    sub[category] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы предсказать вероятность, а не класс требуется `CalibratedClassifierCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "... Processing severe_toxic\n",
      "... Processing obscene\n",
      "... Processing threat\n",
      "... Processing insult\n",
      "... Processing identity_hate\n"
     ]
    }
   ],
   "source": [
    "calib = CalibratedClassifierCV(base_estimator=clf)\n",
    "for category in categories:\n",
    "    print('... Processing {}'.format(category))\n",
    "    calib.fit(train_features, train[category])\n",
    "    prediction = calib.predict_proba(test_features)[:,1]\n",
    "    sub[category] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.548476</td>\n",
       "      <td>0.003502</td>\n",
       "      <td>0.033639</td>\n",
       "      <td>0.002428</td>\n",
       "      <td>0.108342</td>\n",
       "      <td>0.004298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150001</td>\n",
       "      <td>0.019925</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>0.012905</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.011922</td>\n",
       "      <td>0.004098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150002</td>\n",
       "      <td>0.018510</td>\n",
       "      <td>0.002965</td>\n",
       "      <td>0.009980</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.012460</td>\n",
       "      <td>0.003542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150003</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>0.003049</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>0.002659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150004</td>\n",
       "      <td>0.028122</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.011492</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>0.014278</td>\n",
       "      <td>0.003854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id     toxic  severe_toxic   obscene    threat    insult  identity_hate\n",
       "0  150000  0.548476      0.003502  0.033639  0.002428  0.108342       0.004298\n",
       "1  150001  0.019925      0.003077  0.012905  0.001776  0.011922       0.004098\n",
       "2  150002  0.018510      0.002965  0.009980  0.001559  0.012460       0.003542\n",
       "3  150003  0.001833      0.002349  0.003049  0.000978  0.004356       0.002659\n",
       "4  150004  0.028122      0.001997  0.011492  0.001087  0.014278       0.003854"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9643494282892767"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "for train_target in categories:\n",
    "    cv_score = np.mean(cross_val_score(calib, train_features, train[train_target], cv=3, scoring='roc_auc')) \n",
    "    scores.append(cv_score)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный метод показал себя с наилучшей стороны, особенно при параметре `C=0.7`, но к сожалению, лимит загрузок исчерпан)\n",
    " Кстати, что очень странно. При параметре равном 0.01, точность лучше на кросс валидации, однако при сабмите хуже, чем при 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем другие методы для обработки, сначала сделаем необходимые импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import io\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала сделаем функцию, чтобы можно было импортить любую эмбединг модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec(file):\n",
    "    fin = io.open(file, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data\n",
    "model_emb = vec('wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем эмбединг-словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fast = {k: list(v) for k, v in model_emb.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим теперь мешок слов по приципу:<br>\n",
    "Если слово встречается реже 100 раз, значит это очень специфичный термин,\n",
    "используемый в очень маленьком количестве документов, либо же так называемая \"опечатка\", что маловероятно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=100)\n",
    "X_vect = count_vectorizer.fit_transform(X.apply(lambda x: ' '.join(w for w in x))).toarray()\n",
    "X_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составим эмбеддинги c 3 словами с наивысшим значением `tf_idf`. Если слова отсуствуют в эмбеддинг словаре, используем эмбеддинги следующих слов по значению `tf_idf`.<br>\n",
    "На самом деле это оч ресурсозатратный процесс, даже кластер еле справляется, но один раз получилось это запустить. Точность была 92%, что ни в какие рамки не входит)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_dictionary = dict(zip(tf_idf_vectorizer.get_feature_names(), tf_idf_vectorizer.idf_))\n",
    "X_emb_df = pd.DataFrame(columns=range(900))\n",
    "for num in tqdm(range(len(train))):\n",
    "    comment = X[num]\n",
    "    words = [tf_idf_dictionary[word] for word in comment]\n",
    "    important = np.array(comment)[np.argsort(words)[:-10:-1]]\n",
    "    embeddings = np.array([])\n",
    "    non_found = 0\n",
    "    initial = 3\n",
    "    count = 3\n",
    "    for im in important[:count]:\n",
    "        if im in model_fast: \n",
    "            embeddings = np.append(embeddings, np.array(model_fast[word]))\n",
    "        else:\n",
    "            non_found += 1\n",
    "    while non_found != 0 and len(important)//300>count:\n",
    "        im = important_words[count]\n",
    "        if im_word in model_fast:\n",
    "            embeddings = np.append(embeddings, np.array(model_fast[im_word]))\n",
    "            non_found -= 1\n",
    "        else:\n",
    "            continue\n",
    "        count += 1\n",
    "    if len(embeddings) // 300 < initial:\n",
    "        embeddings = np.append(embeddings, np.repeat(np.repeat(0, 300), initial-len(embeddings)//300))\n",
    "    X_emb_df.loc[num] = np.ravel(embeddings)\n",
    "    if (num+1) % 1000 == 0:\n",
    "        X_emb_df.to_csv('DataFrame' + str(num-1000) + '-' + str(num) + '.csv')\n",
    "        print('done')\n",
    "        del X_emb_df\n",
    "        X_emb_df = pd.DataFrame(columns=range(900))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединяем все в дф"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_emb_df = pd.DataFrame(columns=map(str, range(900)))\n",
    "dfs = sorted(filter(lambda x: x.startswith('DataFrame'), os.listdir()), key=lambda x: int(x[x.rfind('-')+1:-4]))\n",
    "for one_df in tqdm(dfs):\n",
    "    X_emb_df = X_emb_df.append(pd.read_csv(one_df).iloc[:, 1:])\n",
    "    print(X_emb_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее соединяем мешок слов и эмбединги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ready = np.hstack((X_vect, X_emb_df.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подводя итоги, хотелось бы сказать, что в данном случае самым эффективным оказался векторизатор `tf-idf`, а также лемматизация с помощью `nltk`. Токенизацию также можно сделать через векторизатор или вместе с лемматизацией `nltk`, тк точность от этого особо не меняется. Во всех случаях ведет себя по-разному.  Стопслова так чистятся во время векторизации. Самыми точными моделями оказались `Logistic Regression` и `Supported Vectors Machine`. Хотелось бы построить нейронку и добить точность до потолка, но это уже только в перспективе)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
